"""
ClinicTriage Model Performance Visualization
=============================================

Comprehensive visualization of model performance including:
- 5-class confusion matrix
- Per-class performance metrics
- Critical case performance (both Exact Accuracy and Detection Rate)
- Binary confusion matrix with TP, FN, FP, TN breakdown
- Model performance summary

This script loads the existing triage_model_mimic_v2.pkl and visualizes all metrics.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.gridspec import GridSpec
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, 
    classification_report,
    confusion_matrix,
    f1_score,
    recall_score,
    precision_score
)
import pickle
from datetime import datetime

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10

print("=" * 70)
print("CLINICTRIAGE MODEL PERFORMANCE VISUALIZATION")
print("=" * 70)

# ============================================================================
# LOAD MODEL AND DATA
# ============================================================================

print("\nğŸ“‚ Loading model and data...")

# Load model
model_file = 'triage_model_mimic_v2_OLD.pkl'
try:
    with open(model_file, 'rb') as f:
        model = pickle.load(f)
    print(f"   âœ… Loaded model from '{model_file}'")
except FileNotFoundError:
    print(f"   âŒ ERROR: '{model_file}' not found!")
    print("   Please ensure the model file exists in the current directory.")
    exit(1)

# Try to load metadata
metadata = None
try:
    with open('model_metadata_mimic_v2_OLD.pkl', 'rb') as f:
        metadata = pickle.load(f)
    print(f"   âœ… Loaded metadata")
except FileNotFoundError:
    print(f"   âš ï¸  No metadata file found, will compute all metrics fresh")

# Load data
data_files = [
    'mimic_patients_10k.csv',
    'mimic_patients_10k_enhanced.csv',
#    'clinictriage_data.csv'
]

df = None
for filename in data_files:
    try:
        df = pd.read_csv(filename)
        print(f"   âœ… Loaded {len(df):,} records from '{filename}'")
        break
    except FileNotFoundError:
        continue

if df is None:
    print("   âŒ ERROR: No data file found!")
    exit(1)

# ============================================================================
# PREPARE FEATURES
# ============================================================================

print("\nğŸ”§ Preparing features...")

# Get feature names from model or metadata
if hasattr(model, 'feature_names_in_'):
    feature_columns = list(model.feature_names_in_)
elif metadata and 'feature_names' in metadata:
    feature_columns = metadata['feature_names']
else:
    # Default feature columns
    feature_columns = [
        'age', 'symptom_severity', 'symptom_duration_hours',
        'heart_rate', 'systolic_bp', 'diastolic_bp',
        'temperature', 'oxygen_saturation',
        'has_red_flag', 'has_chronic_condition', 'high_risk_chronic',
        'hr_abnormal', 'bp_abnormal', 'temp_abnormal', 'spo2_abnormal',
        'vital_abnormalities', 'symptom_acuity',
        'previous_visits', 'gender_encoded', 'onset_encoded'
    ]
    feature_columns = [col for col in feature_columns if col in df.columns]

print(f"   Using {len(feature_columns)} features")

X = df[feature_columns]
y = df['urgency_level']

# Same split as training (random_state=42 for reproducibility)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"   Test set: {len(X_test):,} records")

# Ensure feature order matches model
if hasattr(model, 'feature_names_in_'):
    X_test = X_test[model.feature_names_in_]

# ============================================================================
# GENERATE PREDICTIONS
# ============================================================================

print("\nğŸ“Š Generating predictions...")

y_pred = model.predict(X_test)

# ============================================================================
# CALCULATE ALL METRICS
# ============================================================================

print("\nğŸ“ˆ Calculating metrics...")

# Overall accuracy
overall_accuracy = accuracy_score(y_test, y_pred)
weighted_f1 = f1_score(y_test, y_pred, average='weighted')

# Per-class metrics
class_report = classification_report(y_test, y_pred, output_dict=True)

# 5-class confusion matrix
cm_5class = confusion_matrix(y_test, y_pred)

# ============================================================================
# CRITICAL METRICS - BOTH DEFINITIONS
# ============================================================================

# 1. Critical Exact Accuracy
critical_mask = y_test.isin([1, 2])
y_test_critical = y_test[critical_mask]
y_pred_critical = y_pred[critical_mask.values]
critical_exact_accuracy = accuracy_score(y_test_critical, y_pred_critical)

# Count exact matches
l1_correct = ((y_test == 1) & (y_pred == 1)).sum()
l2_correct = ((y_test == 2) & (y_pred == 2)).sum()
total_critical = critical_mask.sum()

# 2. Critical Detection Rate (Binary)
y_test_binary = (y_test <= 2).astype(int)  # 1 = critical, 0 = non-critical
y_pred_binary = (pd.Series(y_pred) <= 2).astype(int)

# Binary confusion matrix
cm_binary = confusion_matrix(y_test_binary, y_pred_binary)
tn, fp, fn, tp = cm_binary.ravel()

# Detection metrics
critical_detection_rate = recall_score(y_test_binary, y_pred_binary)  # Sensitivity
critical_precision = precision_score(y_test_binary, y_pred_binary)
critical_f1 = f1_score(y_test_binary, y_pred_binary)
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

# OOB Score if available
oob_score = model.oob_score_ if hasattr(model, 'oob_score_') else None

print(f"   âœ… All metrics calculated")

# ============================================================================
# PRINT METRICS TO CONSOLE
# ============================================================================

print("\n" + "=" * 70)
print("METRIC SUMMARY")
print("=" * 70)

print(f"\n   Overall Accuracy:        {overall_accuracy:.1%}")
print(f"   Weighted F1 Score:       {weighted_f1:.1%}")
if oob_score:
    print(f"   OOB Score:               {oob_score:.1%}")

print(f"\n   CRITICAL METRICS:")
print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"   Critical Exact Accuracy: {critical_exact_accuracy:.1%}")
print(f"   â­ Critical Detection:   {critical_detection_rate:.1%}")
print(f"   Critical Precision:      {critical_precision:.1%}")
print(f"   Critical F1:             {critical_f1:.1%}")

print(f"\n   BINARY CONFUSION MATRIX VALUES:")
print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
print(f"   True Positives (TP):     {tp:,} (Critical correctly detected)")
print(f"   False Negatives (FN):    {fn:,} (Critical MISSED - dangerous!)")
print(f"   False Positives (FP):    {fp:,} (Over-triaged)")
print(f"   True Negatives (TN):     {tn:,} (Non-critical correct)")

print(f"\n   DETECTION RATE FORMULA:")
print(f"   Critical Detection = TP / (TP + FN)")
print(f"                      = {tp} / ({tp} + {fn})")
print(f"                      = {tp} / {tp + fn}")
print(f"                      = {critical_detection_rate:.1%}")

# ============================================================================
# CREATE VISUALIZATION
# ============================================================================

print("\nğŸ¨ Creating visualization...")

# Create figure with custom layout
fig = plt.figure(figsize=(16, 14))
fig.suptitle('ClinicTriage Model Performance Analysis\n(triage_model_mimic_v2.pkl)', 
             fontsize=16, fontweight='bold', y=0.98)

# Define grid
gs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.3,
              left=0.06, right=0.94, top=0.92, bottom=0.05)

# Color scheme
colors = {
    'critical': '#E74C3C',      # Red
    'non_critical': '#27AE60',  # Green
    'highlight': '#3498DB',     # Blue
    'warning': '#F39C12',       # Orange
    'neutral': '#95A5A6'        # Gray
}

level_colors = ['#C0392B', '#E74C3C', '#F39C12', '#27AE60', '#2ECC71']
level_names = ['L1 (Critical)', 'L2 (High)', 'L3 (Moderate)', 'L4 (Low)', 'L5 (Non-Urgent)']

# ============================================================================
# PLOT 1: 5-CLASS CONFUSION MATRIX
# ============================================================================

ax1 = fig.add_subplot(gs[0, 0:2])

# Normalize confusion matrix for display
cm_normalized = cm_5class.astype('float') / cm_5class.sum(axis=1)[:, np.newaxis]

# Create heatmap
sns.heatmap(cm_normalized, annot=False, cmap='Blues', ax=ax1,
            xticklabels=['L1', 'L2', 'L3', 'L4', 'L5'],
            yticklabels=['L1', 'L2', 'L3', 'L4', 'L5'],
            cbar_kws={'label': 'Proportion'})

# Add text annotations with both count and percentage
for i in range(5):
    for j in range(5):
        count = cm_5class[i, j]
        pct = cm_normalized[i, j] * 100
        text_color = 'white' if cm_normalized[i, j] > 0.5 else 'black'
        ax1.text(j + 0.5, i + 0.5, f'{count}\n({pct:.0f}%)',
                ha='center', va='center', fontsize=9, color=text_color)

ax1.set_title('5-Class Confusion Matrix', fontweight='bold', pad=10)
ax1.set_xlabel('Predicted Level')
ax1.set_ylabel('Actual Level')

# Add critical zone highlight
rect = plt.Rectangle((0, 0), 2, 2, fill=False, edgecolor=colors['critical'], 
                      linewidth=3, linestyle='--')
ax1.add_patch(rect)
ax1.text(1, -0.3, 'â† Critical Zone', fontsize=9, color=colors['critical'], 
         ha='center', fontweight='bold')

# ============================================================================
# PLOT 2: PER-CLASS PERFORMANCE
# ============================================================================

ax2 = fig.add_subplot(gs[0, 2])

# Extract per-class metrics
classes = ['1', '2', '3', '4', '5']
precisions = [class_report[c]['precision'] for c in classes]
recalls = [class_report[c]['recall'] for c in classes]
f1_scores = [class_report[c]['f1-score'] for c in classes]

x = np.arange(5)
width = 0.25

bars1 = ax2.bar(x - width, precisions, width, label='Precision', color='#3498DB', alpha=0.8)
bars2 = ax2.bar(x, recalls, width, label='Recall', color='#E74C3C', alpha=0.8)
bars3 = ax2.bar(x + width, f1_scores, width, label='F1-Score', color='#2ECC71', alpha=0.8)

ax2.set_xlabel('Urgency Level')
ax2.set_ylabel('Score')
ax2.set_title('Per-Class Performance Metrics', fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(['L1', 'L2', 'L3', 'L4', 'L5'])
ax2.set_ylim(0, 1.1)
ax2.legend(loc='upper right', fontsize=8)
ax2.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, linewidth=1)

# Highlight critical levels
for i in [0, 1]:
    ax2.axvspan(i - 0.4, i + 0.4, alpha=0.1, color=colors['critical'])

# ============================================================================
# PLOT 3: BINARY CONFUSION MATRIX (CRITICAL VS NON-CRITICAL)
# ============================================================================

ax3 = fig.add_subplot(gs[1, 0])

# Create styled binary confusion matrix
binary_labels = ['Non-Critical\n(L3-L5)', 'Critical\n(L1-L2)']
cm_binary_display = np.array([[tn, fp], [fn, tp]])

# Custom colors: green for correct, red for errors
cell_colors = np.array([
    ['#27AE60', '#E74C3C'],  # TN (green), FP (orange-ish)
    ['#C0392B', '#27AE60']   # FN (dark red - dangerous!), TP (green)
])

for i in range(2):
    for j in range(2):
        color = cell_colors[i, j]
        alpha = 0.7 if (i == j) else 0.5
        rect = plt.Rectangle((j, 1-i), 1, 1, facecolor=color, alpha=alpha, edgecolor='white', linewidth=2)
        ax3.add_patch(rect)
        
        # Add text
        value = cm_binary_display[i, j]
        pct = value / cm_binary_display.sum() * 100
        
        # Label for each cell
        if i == 0 and j == 0:
            label = f'TN\n{value:,}\n({pct:.1f}%)'
        elif i == 0 and j == 1:
            label = f'FP\n{value:,}\n({pct:.1f}%)'
        elif i == 1 and j == 0:
            label = f'FN âš ï¸\n{value:,}\n({pct:.1f}%)'
        else:
            label = f'TP âœ“\n{value:,}\n({pct:.1f}%)'
        
        ax3.text(j + 0.5, 1.5 - i, label, ha='center', va='center', 
                fontsize=11, fontweight='bold', color='white')

ax3.set_xlim(0, 2)
ax3.set_ylim(0, 2)
ax3.set_xticks([0.5, 1.5])
ax3.set_xticklabels(['Non-Critical', 'Critical'])
ax3.set_yticks([0.5, 1.5])
ax3.set_yticklabels(['Critical', 'Non-Critical'])
ax3.set_xlabel('Predicted', fontweight='bold')
ax3.set_ylabel('Actual', fontweight='bold')
ax3.set_title('Binary Confusion Matrix\n(Critical Detection)', fontweight='bold')

# ============================================================================
# PLOT 4: CRITICAL DETECTION RATE BREAKDOWN
# ============================================================================

ax4 = fig.add_subplot(gs[1, 1])

# Pie chart showing detection breakdown
detection_data = [tp, fn]
detection_labels = [f'Detected (TP)\n{tp:,}', f'Missed (FN)\n{fn:,}']
detection_colors = [colors['non_critical'], colors['critical']]
explode = (0.02, 0.05)  # Slightly explode the "missed" slice

wedges, texts, autotexts = ax4.pie(
    detection_data, 
    labels=detection_labels,
    colors=detection_colors,
    autopct='%1.1f%%',
    explode=explode,
    startangle=90,
    textprops={'fontsize': 10}
)

# Style the percentage text
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')
    autotext.set_fontsize(12)

ax4.set_title(f'Critical Detection Rate\nâ­ {critical_detection_rate:.1%}', 
              fontweight='bold', fontsize=13)

# Add formula annotation
ax4.text(0, -1.4, f'Formula: TP / (TP + FN) = {tp} / {tp + fn} = {critical_detection_rate:.1%}',
         ha='center', fontsize=9, style='italic')

# ============================================================================
# PLOT 5: CRITICAL METRICS COMPARISON
# ============================================================================

ax5 = fig.add_subplot(gs[1, 2])

# Bar chart comparing the two critical metrics
metrics_names = ['Critical\nExact Accuracy', 'Critical\nDetection Rate']
metrics_values = [critical_exact_accuracy, critical_detection_rate]
bar_colors = [colors['warning'], colors['highlight']]

bars = ax5.barh(metrics_names, metrics_values, color=bar_colors, height=0.5, edgecolor='white')

# Add value labels
for bar, val in zip(bars, metrics_values):
    ax5.text(val + 0.02, bar.get_y() + bar.get_height()/2, 
             f'{val:.1%}', va='center', fontweight='bold', fontsize=12)

ax5.set_xlim(0, 1.15)
ax5.set_xlabel('Score')
ax5.set_title('Critical Accuracy Metrics Comparison', fontweight='bold')
ax5.axvline(x=0.8, color='gray', linestyle='--', alpha=0.5, label='80% threshold')

# Add explanation text
explanation = (
    "Exact: Predicted exact level (L1â†’L1, L2â†’L2)\n"
    "Detection: Caught as critical (L1 or L2)"
)
ax5.text(0.5, -0.5, explanation, transform=ax5.transAxes, fontsize=8,
         ha='center', style='italic', color='gray')

# ============================================================================
# PLOT 6: MODEL PERFORMANCE SUMMARY
# ============================================================================

ax6 = fig.add_subplot(gs[2, 0])
ax6.axis('off')

# Create summary text
summary_text = f"""
MODEL PERFORMANCE SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Overall Metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Overall Accuracy:     {overall_accuracy:.1%}
  Weighted F1 Score:    {weighted_f1:.1%}
  OOB Score:            {oob_score:.1%} {"" if oob_score else "N/A"}

Critical Case Metrics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Exact Accuracy:       {critical_exact_accuracy:.1%}
  â­ Detection Rate:    {critical_detection_rate:.1%}
  Precision:            {critical_precision:.1%}
  F1 Score:             {critical_f1:.1%}
  Specificity:          {specificity:.1%}

Test Set Statistics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total samples:        {len(y_test):,}
  Critical cases:       {total_critical:,} ({total_critical/len(y_test)*100:.1f}%)
  Non-critical cases:   {len(y_test) - total_critical:,}
"""

ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes,
         fontsize=10, fontfamily='monospace', verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='#F8F9FA', edgecolor='#DEE2E6'))

# ============================================================================
# PLOT 7: DETECTION FORMULA BREAKDOWN
# ============================================================================

ax7 = fig.add_subplot(gs[2, 1])
ax7.axis('off')

formula_text = f"""
CRITICAL DETECTION RATE FORMULA
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    True Positives (TP)
Detection Rate = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  True Positives + False Negatives


Component Values:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TP (Critical detected):     {tp:,}
  FN (Critical MISSED):       {fn:,}
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total actual critical:      {tp + fn:,}


Calculation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Detection Rate = {tp} / ({tp} + {fn})
                 = {tp} / {tp + fn}
                 = {critical_detection_rate:.4f}
                 = {critical_detection_rate:.1%}  â­


Clinical Interpretation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Of {tp + fn:,} critical patients, we correctly
  identified {tp:,} as needing urgent attention.
  
  {fn:,} critical patients were missed.
"""

ax7.text(0.05, 0.95, formula_text, transform=ax7.transAxes,
         fontsize=9, fontfamily='monospace', verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='#EBF5FB', edgecolor='#3498DB'))

# ============================================================================
# PLOT 8: PREDICTED VS ACTUAL DISTRIBUTION
# ============================================================================

ax8 = fig.add_subplot(gs[2, 2])

# Bar chart comparing actual vs predicted distribution
levels = ['L1', 'L2', 'L3', 'L4', 'L5']
actual_counts = [(y_test == i).sum() for i in range(1, 6)]
predicted_counts = [(y_pred == i).sum() for i in range(1, 6)]

x = np.arange(5)
width = 0.35

bars_actual = ax8.bar(x - width/2, actual_counts, width, label='Actual', 
                       color=level_colors, alpha=0.7, edgecolor='black')
bars_pred = ax8.bar(x + width/2, predicted_counts, width, label='Predicted',
                     color=level_colors, alpha=0.4, edgecolor='black', hatch='//')

ax8.set_xlabel('Urgency Level')
ax8.set_ylabel('Count')
ax8.set_title('Predicted vs Actual Distribution', fontweight='bold')
ax8.set_xticks(x)
ax8.set_xticklabels(levels)
ax8.legend()

# Add count labels
for bar in bars_actual:
    height = bar.get_height()
    ax8.text(bar.get_x() + bar.get_width()/2., height,
             f'{int(height)}', ha='center', va='bottom', fontsize=8)

# ============================================================================
# SAVE FIGURE
# ============================================================================

# Add timestamp
fig.text(0.99, 0.01, f'Generated: {datetime.now().strftime("%Y-%m-%d %H:%M")}',
         ha='right', fontsize=8, color='gray')

# Save figures
output_file = 'model_performance_v2_complete.png'
output_file_hires = 'model_performance_v2_complete_hires.png'

plt.savefig(output_file, dpi=150, bbox_inches='tight', facecolor='white')
plt.savefig(output_file_hires, dpi=300, bbox_inches='tight', facecolor='white')

print(f"\nâœ… Visualization saved as '{output_file}'")
print(f"âœ… High-res version saved as '{output_file_hires}'")

plt.close()

# ============================================================================
# FINAL CONSOLE SUMMARY
# ============================================================================

print("\n" + "=" * 70)
print("ğŸ¯ VISUALIZATION COMPLETE")
print("=" * 70)

print(f"""
   Files created:
   â€¢ {output_file} (standard resolution)
   â€¢ {output_file_hires} (high resolution for presentations)

   KEY METRICS VERIFIED:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Overall Accuracy:         {overall_accuracy:.1%}
   Critical Exact Accuracy:  {critical_exact_accuracy:.1%}
   â­ Critical Detection:    {critical_detection_rate:.1%}

   DETECTION RATE COMPONENTS:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   True Positives (TP):      {tp:,}  (Critical cases correctly detected)
   False Negatives (FN):     {fn:,}  (Critical cases MISSED)
   
   Formula: TP / (TP + FN) = {tp} / {tp + fn} = {critical_detection_rate:.1%}

   INTERPRETATION:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   The model correctly identifies {critical_detection_rate:.1%} of all critical
   patients (L1 + L2) as needing urgent attention.
   
   This is the "89.3%" metric from your original training.
""")

print("=" * 70)
